{\rtf1\ansi\ansicpg1252\deff0\deflang2057{\fonttbl{\f0\fswiss\fcharset0 Arial;}}
{\*\generator Msftedit 5.41.21.2508;}\viewkind4\uc1\pard\f0\fs20 Final Year Project\par
\par
Since I plan to spend the next few weeks working on it heavily, I thought it would be a good time to write about my FYP.\par
\par
I'm writing an AI to play Texas Holdem Poker; specifically Heads-Up Limit (no-limit is much more difficult for computer players; more possible moves means more states, more possibilities, and more balls-to-the-wall calls of your opponent's daring all-in push because you think you noticed his eyebrow twitch - easy for Hollywood scriptwriters to make exciting, hard for an algorithm to reason about).\par
\par
First off, some background. Poker is an interesting game to write an AI for, as it's both an indeterministic and an imperfect information game - it involves both elements of chance and of suprise.\par
\par
A lot of games - Chess, Go, Tic-Tac-Toe - are, by contrast, deterministic, perfect information games, which means the good old Minimax algorithm works well. What's my best move, assuming my opponent will then make his best move and then I'll make my best move, and so on, until someone wins? Of course, with complex games like Chess there's too many states to compute the entire game tree, so you only look so far ahead, pruning uninteresting branches of the tree, and have an evaluation function to determine the value of intermediate states. Still, it's a very effective approach.\par
\par
With indeterministic games like Backgammon, you have to switch from Minimax to Expectimax to account for the element of chance, but the increase in complexity is not that great. However, add imperfect information, and things get much harder - in Poker, there's often simply no way to know what the right move is until the hand is over and everyone's cards are revealed. You also can't assume your opponent will play the optimal move, since you don't know what that is, so you have to try to figure out probabilities for their actions - this is known as opponent modelling, and is going to be one of the things my FYP will focus on. Basically, I'm going to implement an algorithm called Miximix, which requires a decent opponent modelling function; I'll then try various Machine Learning techniques to see which is the best for this job.\par
\par
To explain Miximix, I'll need to explain two types of strategy for Poker and games like it; equilibrium and exploitative. I'll use Rock-Paper-Scissors as an example, since it's a simpler game.\par
\par
It's clear that if you play Rock-Paper-Scissors randomly, making each move with an exactly 1/3 chance, then in the long run you can't lose. This is an equilibirum strategy; in game-theoretic terms, it's a Nash equilibrium, where if both players follow this strategy neither has any incentive to change. However, although you can't lose, you also can't win; your expectation for every game is precisely 0.\par
\par
Imagine if, filled with pride and hubris at discovering the game's optimal strategy, you enter yourself in a three-player Rock-Paper-Scissor tournament. Your first opponent is a pale and lanky fellow named Edward, who only plays Scissors; after 1000 rounds, luck has slightly been on your side and you are 19 points ahead. Your next opponent is Dwayne, a tanned and musclebound chap, who only plays Rock; after 1000 rounds with him, you find yourself up 37 points. You then sit in the audience to watch Dwayne play Edward, both players sticking steadfastly to their chosen strategies; Dwayne of course wins all 1000 rounds, and is declared overall victor of the tounament.\par
\par
Though this is a contrived example, it shows the trade-off between exploitative and equilibrium strategies. Dwayne's all-rock gambit defeats Edward's all-scissors tactics, but could themselves be defeated by someone playing all-paper. Strategies designed to exploit an opponent's weaknesses can always themselves be exploited; this is true even for clever players that adapt themselves to an opponent as they play (play one strategy until they start trying to exploit that strategy, then switch to a different startegy that exploits their new strategy). In contast, equilibrium strategies are safe in that they don't tend to lose by very much, but they're bad for games where you're not just trying to win, you're trying to win by a lot.\par
\par
Miximix is an exploitative strategy. It's a generalisation of Minimax where both you and your opponent follow a mixed strategy, meaning that you treat player nodes as chance nodes, where the probabilities are the player's probabilities of calling, raising, or folding. (A special case is Miximax, where you assume your opponent still follows a mixed strategy, but you yourself always take the best action). It's not the best poker algorithm in existence, but has the advantage (from my POV) of being relatively simple to implement. \par
\par
Determining what exactly your opponent's action probabilities are is non-trivial, and is going to be the focus of my FYP - as I mentioned above, I'll experiment with various Machine Learning algorithms for this job. The basic approach I'll take is to treat it as a supervised learning problem where the inputs are the previous actions that hand (and possibly the cards on the board) and the outputs are the probabilities for each action. There's a few problems with this (for example, in a real game you can't play badly for hundreds of hands while your algo collects training data) but I do have a few ideas to get around them, time permitting. Anyway, enough theory, time to talk about the implementation.\par
\par
As well as writing a poker player, I also had to write a poker dealer, which enforces the rules of Texas Holdem between two opponents. Although the AI is going to be written in Java - as well as being reasonably performant, it has a nice Machine Learning library, Weka - I decided to write the dealer program in Ruby. The rationale was that the dealer would be fairly complicated to write, but performance wasn't much of an issue (the amount of computation done by the dealer likely being a tiny fraction of that done by the AI). Since I'm -more familiar with Ruby, and it's a terser language, I figured using it would mean I finished this part of the project more quickly. The dealer and the players communicate via TCP.\par
\par
I also wrote a little web server in Ruby to provide a nice interface for human players, as well as a terminal-based client and a few simple Ruby bots for testing purposes. I'll try and put it online at some point, though there's a few problems to iron out first (I wrote it for demonstration purposes, so for now it can't handle multiple connections).\par
\par
Anyway, writing the dealer was time-consuming, though not especially interesting (turns out there's a lot of special cases in the rules of Poker). The most difficult part was determining which of two hands was the winner at showdown; ie, choosing the best 5-card combination from all 7 cards. My approach was to enumerate all 21 five-card-combinations, then do a bunch of sorting and comparing cards to determinine which combination was the best and what exact rank it represented.\par
\par
Since this took quite a lot of time, today I was annoyed to discover that it had been done before, and much better than I had (several thousands of times faster). http://www.codingthewheel.com/archives/poker-hand-evaluator-roundup details several approaches - one of the fastest assigns every card to a unique prime number, multiplies them together to get a unique number for every hand, then performs a bunch of complicated bitwise operations to determine the rank. The prize, however, goes to the Two Plus Two evaluator; a giant lookup table of 32,487,834 entries, with several clever optimisations. I discovered that approaches like mine are known as "naive". :-( It's not important for the dealer program, which only needs to evaluate hand ranks once per round, but the AI may need to consider millions of leaf nodes for every action it takes, so I'm glad I discovered this now.\par
\par
Right now I'm reading through some papers to get a firm grasp on the theory and work out my approach. Hopefully I'll soon be ready to start work on the AI and have more to write about!\par
}
 